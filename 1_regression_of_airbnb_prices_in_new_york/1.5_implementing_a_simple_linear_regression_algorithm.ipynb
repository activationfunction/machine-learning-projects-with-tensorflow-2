{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"1.5_implementing_a_simple_linear_regression_algorithm.ipynb","provenance":[{"file_id":"https://github.com/PacktPublishing/Machine-Learning-Projects-with-TensorFlow-2.0/blob/master/Section%201/Video%201.5.ipynb","timestamp":1615879502854}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"bFT_fWu-7Fhn"},"source":["# 1.5. Implementing a simple Linear Regression Algorithm"]},{"cell_type":"markdown","metadata":{"id":"pL9vJ-W47Fhr"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"Y6UYXL5v7Fhs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616033520717,"user_tz":-480,"elapsed":3394,"user":{"displayName":"Jazon Samillano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38FD5jYw0f-Aq4w9GYw6VLsoIdOlo8btjIQwQ=s64","userId":"00135806175043031109"}},"outputId":"91161d37-3e40-4682-f29c-e81776f15ca5"},"source":["import numpy as np\n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import warnings\n","import seaborn as sb\n","\n","warnings.filterwarnings('ignore')\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m-AXAC0l7Fhs"},"source":["## Load data and take a look at it"]},{"cell_type":"code","metadata":{"id":"euoIPRyA7Fht","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"ok","timestamp":1616033520718,"user_tz":-480,"elapsed":3389,"user":{"displayName":"Jazon Samillano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38FD5jYw0f-Aq4w9GYw6VLsoIdOlo8btjIQwQ=s64","userId":"00135806175043031109"}},"outputId":"a387984a-cdca-4f35-9a07-3c073c49c4d7"},"source":["data = pd.read_csv('https://storage.googleapis.com/activation-function/csv/airbnb_new_york.csv').sample(frac=1)\n","data.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>host_id</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>price</th>\n","      <th>minimum_nights</th>\n","      <th>number_of_reviews</th>\n","      <th>reviews_per_month</th>\n","      <th>calculated_host_listings_count</th>\n","      <th>availability_365</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>4.889500e+04</td>\n","      <td>4.889500e+04</td>\n","      <td>48895.000000</td>\n","      <td>48895.000000</td>\n","      <td>48895.000000</td>\n","      <td>48895.000000</td>\n","      <td>48895.000000</td>\n","      <td>38843.000000</td>\n","      <td>48895.000000</td>\n","      <td>48895.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.901714e+07</td>\n","      <td>6.762001e+07</td>\n","      <td>40.728949</td>\n","      <td>-73.952170</td>\n","      <td>152.720687</td>\n","      <td>7.029962</td>\n","      <td>23.274466</td>\n","      <td>1.373221</td>\n","      <td>7.143982</td>\n","      <td>112.781327</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.098311e+07</td>\n","      <td>7.861097e+07</td>\n","      <td>0.054530</td>\n","      <td>0.046157</td>\n","      <td>240.154170</td>\n","      <td>20.510550</td>\n","      <td>44.550582</td>\n","      <td>1.680442</td>\n","      <td>32.952519</td>\n","      <td>131.622289</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>2.539000e+03</td>\n","      <td>2.438000e+03</td>\n","      <td>40.499790</td>\n","      <td>-74.244420</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.010000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>9.471945e+06</td>\n","      <td>7.822033e+06</td>\n","      <td>40.690100</td>\n","      <td>-73.983070</td>\n","      <td>69.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.190000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.967728e+07</td>\n","      <td>3.079382e+07</td>\n","      <td>40.723070</td>\n","      <td>-73.955680</td>\n","      <td>106.000000</td>\n","      <td>3.000000</td>\n","      <td>5.000000</td>\n","      <td>0.720000</td>\n","      <td>1.000000</td>\n","      <td>45.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2.915218e+07</td>\n","      <td>1.074344e+08</td>\n","      <td>40.763115</td>\n","      <td>-73.936275</td>\n","      <td>175.000000</td>\n","      <td>5.000000</td>\n","      <td>24.000000</td>\n","      <td>2.020000</td>\n","      <td>2.000000</td>\n","      <td>227.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>3.648724e+07</td>\n","      <td>2.743213e+08</td>\n","      <td>40.913060</td>\n","      <td>-73.712990</td>\n","      <td>10000.000000</td>\n","      <td>1250.000000</td>\n","      <td>629.000000</td>\n","      <td>58.500000</td>\n","      <td>327.000000</td>\n","      <td>365.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ...  availability_365\n","count  4.889500e+04  ...      48895.000000\n","mean   1.901714e+07  ...        112.781327\n","std    1.098311e+07  ...        131.622289\n","min    2.539000e+03  ...          0.000000\n","25%    9.471945e+06  ...          0.000000\n","50%    1.967728e+07  ...         45.000000\n","75%    2.915218e+07  ...        227.000000\n","max    3.648724e+07  ...        365.000000\n","\n","[8 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"TmL8PZuL7Fht"},"source":["## Data preprocessing"]},{"cell_type":"code","metadata":{"id":"17tlTPKw7Fht","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616033521049,"user_tz":-480,"elapsed":2632,"user":{"displayName":"Jazon Samillano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38FD5jYw0f-Aq4w9GYw6VLsoIdOlo8btjIQwQ=s64","userId":"00135806175043031109"}},"outputId":"8da9e134-906d-4c49-9429-8f12f7aa8561"},"source":["features = data[['neighbourhood_group', 'room_type', 'minimum_nights', 'number_of_reviews', \n","                 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\n","#print(features.isna().sum())\n","features['reviews_per_month'] = features['reviews_per_month'].fillna(0)\n","#print(features.isna().sum())\n","\n","onehot_neighborhood_group = pd.get_dummies(features['neighbourhood_group'])\n","onehot_room_type = pd.get_dummies(features['room_type'])\n","#print(onehot_room_type)\n","\n","features = features.drop(columns=['neighbourhood_group', 'room_type'])\n","features = pd.concat([features, onehot_neighborhood_group, onehot_room_type], axis=1)\n","#print(features.head())\n","\n","targets = data['price']\n","\n","train_size = int(0.7*len(data))\n","X_train, X_test = features.values[:train_size, :], features.values[train_size:, :]\n","y_train, y_test = targets.values[:train_size], targets.values[train_size:]\n","\n","print('X_train[0] length:', len(X_train[0]))\n","print('X_train shape:    ', X_train.shape, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train[0] length: 13\n","X_train shape:     (34226, 13) \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uj1pnjw7HG6t","executionInfo":{"status":"ok","timestamp":1616033522227,"user_tz":-480,"elapsed":661,"user":{"displayName":"Jazon Samillano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38FD5jYw0f-Aq4w9GYw6VLsoIdOlo8btjIQwQ=s64","userId":"00135806175043031109"}},"outputId":"4e20f944-73ad-43aa-d4ff-e094fc220a3d"},"source":["X_train[0], y_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 1.  , 15.  ,  0.44,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,\n","         0.  ,  0.  ,  1.  ,  0.  ]), 50)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"-kAXFMjd7Fhu"},"source":["## Data visualization and analysis"]},{"cell_type":"markdown","metadata":{"id":"LQ3jNUnAFxYg"},"source":["Will be completed at the next section"]},{"cell_type":"markdown","metadata":{"id":"lJE0QREK7Fhu"},"source":["## The TensorFlow 2 Machine Learning Approaches"]},{"cell_type":"markdown","metadata":{"id":"huUiBi7K7Fhu"},"source":["### Linear Regression"]},{"cell_type":"code","metadata":{"id":"LvTsiAYF7Fhv"},"source":["class LinearModel:\n","    def __init__(self):\n","        # y_pred = W*X + b\n","        self.initializer = tf.keras.initializers.GlorotUniform()\n","    \n","    def loss(self, y, y_pred):\n","        return tf.reduce_mean(tf.abs(y - y_pred))\n","    \n","    def train(self, X, y, lr=0.00001, epochs=20, verbose=True):\n","            \n","        X = np.asarray(X, dtype=np.float32)\n","        y = np.asarray(y, dtype=np.float32).reshape((-1, 1)) # [1,2,3,4] -> [[1],[2],[3],[4]]\n","        \n","        self.W = tf.Variable(\n","            initial_value=self.initializer(shape=(len(X[0]), 1), dtype='float32'))\n","        self.b = tf.Variable(\n","            initial_value=self.initializer(shape=(1,), dtype='float32'))\n","\n","        def train_step():\n","            with tf.GradientTape() as t:\n","                current_loss = self.loss(y, self.predict(X))\n","\n","            dW, db = t.gradient(current_loss, [self.W, self.b])\n","            self.W.assign_sub(lr * dW) # W -= lr * dW\n","            self.b.assign_sub(lr * db)\n","            \n","            return current_loss\n","        \n","        for epoch in range(epochs):\n","            current_loss = train_step()\n","            if verbose:\n","                print(f'Epoch {epoch}: Loss: {current_loss.numpy()}') # <3 eager execution\n","                \n","    def predict(self, X):\n","        # [a, b] x [b, c]\n","        # X->[n_instances, n_features] dot W->[n_features, 1]\n","        return tf.matmul(X, self.W) + self.b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_oZJ2uX7Fhv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616033560126,"user_tz":-480,"elapsed":1255,"user":{"displayName":"Jazon Samillano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38FD5jYw0f-Aq4w9GYw6VLsoIdOlo8btjIQwQ=s64","userId":"00135806175043031109"}},"outputId":"2fca19b7-d802-4884-ad61-d6f4a24e8b8b"},"source":["model = LinearModel()\n","model.train(X_train, y_train, epochs=100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0: Loss: 209.1935577392578\n","Epoch 1: Loss: 209.0630645751953\n","Epoch 2: Loss: 208.93255615234375\n","Epoch 3: Loss: 208.8020782470703\n","Epoch 4: Loss: 208.67161560058594\n","Epoch 5: Loss: 208.5411376953125\n","Epoch 6: Loss: 208.4106903076172\n","Epoch 7: Loss: 208.2802734375\n","Epoch 8: Loss: 208.1498260498047\n","Epoch 9: Loss: 208.01942443847656\n","Epoch 10: Loss: 207.88905334472656\n","Epoch 11: Loss: 207.75865173339844\n","Epoch 12: Loss: 207.62828063964844\n","Epoch 13: Loss: 207.49789428710938\n","Epoch 14: Loss: 207.36752319335938\n","Epoch 15: Loss: 207.23716735839844\n","Epoch 16: Loss: 207.10679626464844\n","Epoch 17: Loss: 206.97642517089844\n","Epoch 18: Loss: 206.84605407714844\n","Epoch 19: Loss: 206.71568298339844\n","Epoch 20: Loss: 206.58534240722656\n","Epoch 21: Loss: 206.45497131347656\n","Epoch 22: Loss: 206.3246307373047\n","Epoch 23: Loss: 206.1942596435547\n","Epoch 24: Loss: 206.06393432617188\n","Epoch 25: Loss: 205.93359375\n","Epoch 26: Loss: 205.80325317382812\n","Epoch 27: Loss: 205.67295837402344\n","Epoch 28: Loss: 205.54266357421875\n","Epoch 29: Loss: 205.41236877441406\n","Epoch 30: Loss: 205.2820587158203\n","Epoch 31: Loss: 205.15179443359375\n","Epoch 32: Loss: 205.02157592773438\n","Epoch 33: Loss: 204.89137268066406\n","Epoch 34: Loss: 204.76113891601562\n","Epoch 35: Loss: 204.63096618652344\n","Epoch 36: Loss: 204.5007781982422\n","Epoch 37: Loss: 204.37059020996094\n","Epoch 38: Loss: 204.24041748046875\n","Epoch 39: Loss: 204.1102294921875\n","Epoch 40: Loss: 203.98007202148438\n","Epoch 41: Loss: 203.84994506835938\n","Epoch 42: Loss: 203.71981811523438\n","Epoch 43: Loss: 203.58969116210938\n","Epoch 44: Loss: 203.45962524414062\n","Epoch 45: Loss: 203.32958984375\n","Epoch 46: Loss: 203.1995086669922\n","Epoch 47: Loss: 203.0694580078125\n","Epoch 48: Loss: 202.93943786621094\n","Epoch 49: Loss: 202.8094024658203\n","Epoch 50: Loss: 202.6793670654297\n","Epoch 51: Loss: 202.54937744140625\n","Epoch 52: Loss: 202.41944885253906\n","Epoch 53: Loss: 202.28955078125\n","Epoch 54: Loss: 202.15963745117188\n","Epoch 55: Loss: 202.02976989746094\n","Epoch 56: Loss: 201.89993286132812\n","Epoch 57: Loss: 201.77008056640625\n","Epoch 58: Loss: 201.64027404785156\n","Epoch 59: Loss: 201.5104522705078\n","Epoch 60: Loss: 201.380615234375\n","Epoch 61: Loss: 201.25083923339844\n","Epoch 62: Loss: 201.12103271484375\n","Epoch 63: Loss: 200.99130249023438\n","Epoch 64: Loss: 200.8616180419922\n","Epoch 65: Loss: 200.73190307617188\n","Epoch 66: Loss: 200.60218811035156\n","Epoch 67: Loss: 200.47247314453125\n","Epoch 68: Loss: 200.34283447265625\n","Epoch 69: Loss: 200.2132110595703\n","Epoch 70: Loss: 200.0836181640625\n","Epoch 71: Loss: 199.95401000976562\n","Epoch 72: Loss: 199.8244171142578\n","Epoch 73: Loss: 199.69482421875\n","Epoch 74: Loss: 199.56524658203125\n","Epoch 75: Loss: 199.43563842773438\n","Epoch 76: Loss: 199.3060760498047\n","Epoch 77: Loss: 199.176513671875\n","Epoch 78: Loss: 199.0470428466797\n","Epoch 79: Loss: 198.9175567626953\n","Epoch 80: Loss: 198.78811645507812\n","Epoch 81: Loss: 198.6586456298828\n","Epoch 82: Loss: 198.5292205810547\n","Epoch 83: Loss: 198.3997802734375\n","Epoch 84: Loss: 198.27040100097656\n","Epoch 85: Loss: 198.1409912109375\n","Epoch 86: Loss: 198.0115966796875\n","Epoch 87: Loss: 197.88221740722656\n","Epoch 88: Loss: 197.7528533935547\n","Epoch 89: Loss: 197.62350463867188\n","Epoch 90: Loss: 197.49415588378906\n","Epoch 91: Loss: 197.36480712890625\n","Epoch 92: Loss: 197.23548889160156\n","Epoch 93: Loss: 197.10617065429688\n","Epoch 94: Loss: 196.9768829345703\n","Epoch 95: Loss: 196.84762573242188\n","Epoch 96: Loss: 196.7183380126953\n","Epoch 97: Loss: 196.58908081054688\n","Epoch 98: Loss: 196.45980834960938\n","Epoch 99: Loss: 196.33059692382812\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TmI0fh_A7Fhv"},"source":["## Conclusions"]},{"cell_type":"markdown","metadata":{"id":"C8lwcfuyGvpu"},"source":["Will be completed at the next section"]}]}